# 4.7 并行执行

OLAP 场景的 SQL 请求需要分析大量的数据，并且用户往往希望能够尽快地给出结果。单行的串行执行能力有限，为了能够尽快地给出结果，充分利用系统资源，并行执行非常关键。

OceanBase 在很早之前就实现了并行执行，也可以支持大规模的高并发处理。在 TPCH 30TB 的标准测试场景中，OceanBase 用 64 台机器，5120 个 CPU 超线程，同时去服务每一个用户请求，本来需要执行几十分钟的用户请求，OceanBase 在几秒钟内就可以完成。

接下来介绍并行执行具体怎么实现。当执行引擎接收到一个需要并行执行的计划时，首先会将执行计划划分为多个 DFO（data flow operator）。其实每个 DFO 就是一个子计划，需要完成一定的执行任务。

![示例](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/kernel-advanced/V1.0.0/zh-CN/4.oceanbase-sql-engine/9.parallel-execution-01.png)

比如上图示例，t1，t2 表进行 join，join 键是 t1.c1 和 t2.c1，使用的算子是 Hash Join 算子。从上图可以看到划分了两个 DFO：DFO0 和 DFO1。DFO0 主要是扫描 t1 表数据发送给 DFO1，DFO1 在接收到 DFO0 的数据之后，首先会根据这些数据创建一个哈希表，然后开始扫描 t2 的数据并进行 prob，之后将结果向上传递。

拆分完这两个 DFO 之后，调度器就会开始调度 DFO。因为调度是以 DFO 为单位进行的，并且会把父子 DFO 同时调度（这里的 DFO0 和 DFO1 就是一个父子调度）。所以调度就会为 DFO0 和 DFO1 分别分配多个线程来处理 DFO0 和 DFO1 的任务，从图中可以看到 DFO0 和 DFO1 分别拉起了三个 worker 线程来处理对应的任务。DFO0 的三个线程主要是在处理扫描 T1 的数据。T1 中可以看到橙色部分有多个任务，每一个 worker 线程会扫描 T1 的部分数据，然后将数据扫描后发送给 DFO1 对应的三个 worker 线程。

DFO1 的三个 worker 线程分别会处理 T2 表中不同分区的数据，并且对于 DFO0 里面传输过来的数据，首先会根据这些数据创建哈希表，之后读取 DFO1 对应的线程中 T2 的数据，然后进行 prob。由于 DFO1 的每个线程只处理 T2 表中的部分数据，所以就需要 DFO0 将数据传输给 DFO1 中线程时，确保 join 键相同的数据能对应发送到同一个线程中，结合示例，也就是 T1 里面 c1 的数据能够分别发送到 T2 中与 c1 的 join 相同的同一个线程里面去。这样才能保证整体 join 的结果不会出错。

这里从 DFO0 到 DFO1 进行的数据传输的过程叫作 [数据重分布](#数据重分布)。上图可以看到整个的执行过程都是并发处理，这样可以较大程度上提升处理的效率。

## 调度

我们并行执行的调度其实是前文说到的父子 DFO 并行调度的一个过程，中间数据的传输是不会写出一个结果的，即是一个流式传输。

调度还有一个两级调度。两级调度是说主控节点会负责全局调度（QC 调度），各个执行节点会负责本机的一个调度（SQC 调度）。比如，存在一个 DFO 可能会需要在多台机器上执行，每台机器上又有多个线程并行来处理该 DFO。这时先将这个 DFO 调度到每台机器，然后 SQC 会去为这个 DFO 任务分配多个线程，之后并发执行。

调度方式如下：

* 成对调度，有父子关系的 DFO 同时调度。

* 对于左深树，从左往右、自底向上进行调度。

这里有一个多 DFO 调度顺序的例子。

![示例](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/kernel-advanced/V1.0.0/zh-CN/4.oceanbase-sql-engine/9.parallel-execution-02.png)

可以看到，如果 DFO 调度的树是这样的，首先会调度 DFO1 和 DFO0，执行完之后再调度 DFO1 和 DFO2，最后再去调度 DFO1 和 DFO3。

## 数据重分布

在数据重分布上，OceanBase 支持多种数据重分布，包括 Hash、Broadcast、Broadcast2Host、Pkey。

* Hash

  按 join 键、group by 列或 distinct 列执行 hash，再按 worker 数取模，将相同 hash 值的行发送到一个 worker。

* Broadcast

  将左表的全部数据广播到所有 worker，各个 worker 各自执行 join。

* Broadcast2Host

  对于 inner nest loop join，将左表数据广播到右表所在的机器（每个机器一份），如果一台机器上有多个 worker，则为左表每一行数据采用 round robin 的方式选择一个 worker。执行 join 的 worker，收到左表的数据后，将这行数据与该机器所有 partition 做 nest loop join。

* PKey

  左表按右表的分区键做分发，属于同一个分区的行发送到同一个 worker。
